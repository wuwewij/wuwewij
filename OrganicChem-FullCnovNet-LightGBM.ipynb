{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP5sY3urxHIA4PcASHmidE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wuwewij/wuwewij/blob/main/OrganicChem-FullCnovNet-LightGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PaYywVIqhtaA",
        "outputId": "02f068af-4f33-4824-c44d-48b92b8bd059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1: (None, 16, 16, 16) (None, 16) (None, 16, 16, 16) (None, 16)\n",
            "x1_weight: (None, 16) KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='dense_78/Sigmoid:0', description=\"created by layer 'dense_78'\") (None, 16)\n",
            "x1_weight: (None, 1, 1, 16) KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 16), dtype=tf.float32, name=None), name='reshape_63/Reshape:0', description=\"created by layer 'reshape_63'\") (None, 1, 1, 16)\n",
            "x1_weighted: (None, 16, 16, 16) KerasTensor(type_spec=TensorSpec(shape=(None, 16, 16, 16), dtype=tf.float32, name=None), name='tf.math.multiply_63/Mul:0', description=\"created by layer 'tf.math.multiply_63'\") (None, 16, 16, 16)\n",
            "(None, 16, 16, 16)\n",
            "Epoch 1/100\n",
            "23/23 [==============================] - 7s 70ms/step - loss: 1129.8524 - mse: 1129.8524 - val_loss: 816.5544 - val_mse: 816.5544\n",
            "Epoch 2/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 610.5110 - mse: 610.5110 - val_loss: 518.1218 - val_mse: 518.1218\n",
            "Epoch 3/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 410.5887 - mse: 410.5887 - val_loss: 331.9673 - val_mse: 331.9673\n",
            "Epoch 4/100\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 294.8459 - mse: 294.8459 - val_loss: 267.1452 - val_mse: 267.1452\n",
            "Epoch 5/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 235.4025 - mse: 235.4025 - val_loss: 213.0617 - val_mse: 213.0617\n",
            "Epoch 6/100\n",
            "23/23 [==============================] - 1s 64ms/step - loss: 198.8319 - mse: 198.8319 - val_loss: 170.9398 - val_mse: 170.9398\n",
            "Epoch 7/100\n",
            "23/23 [==============================] - 2s 90ms/step - loss: 175.8406 - mse: 175.8406 - val_loss: 221.0092 - val_mse: 221.0092\n",
            "Epoch 8/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 175.1505 - mse: 175.1505 - val_loss: 139.6451 - val_mse: 139.6451\n",
            "Epoch 9/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 145.4104 - mse: 145.4104 - val_loss: 184.5592 - val_mse: 184.5592\n",
            "Epoch 10/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 146.2236 - mse: 146.2236 - val_loss: 178.7135 - val_mse: 178.7135\n",
            "Epoch 11/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 131.8705 - mse: 131.8705 - val_loss: 127.9839 - val_mse: 127.9839\n",
            "Epoch 12/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 116.6650 - mse: 116.6650 - val_loss: 118.5986 - val_mse: 118.5986\n",
            "Epoch 13/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 99.4377 - mse: 99.4377 - val_loss: 121.8834 - val_mse: 121.8834\n",
            "Epoch 14/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 92.5417 - mse: 92.5417 - val_loss: 107.4178 - val_mse: 107.4178\n",
            "Epoch 15/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 83.1313 - mse: 83.1313 - val_loss: 131.8842 - val_mse: 131.8842\n",
            "Epoch 16/100\n",
            "23/23 [==============================] - 2s 86ms/step - loss: 94.3747 - mse: 94.3747 - val_loss: 105.3739 - val_mse: 105.3739\n",
            "Epoch 17/100\n",
            "23/23 [==============================] - 1s 61ms/step - loss: 70.1007 - mse: 70.1007 - val_loss: 80.3284 - val_mse: 80.3284\n",
            "Epoch 18/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 62.3671 - mse: 62.3671 - val_loss: 86.3746 - val_mse: 86.3746\n",
            "Epoch 19/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 66.4130 - mse: 66.4130 - val_loss: 107.9144 - val_mse: 107.9144\n",
            "Epoch 20/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 55.0843 - mse: 55.0843 - val_loss: 59.8752 - val_mse: 59.8752\n",
            "Epoch 21/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 57.7549 - mse: 57.7549 - val_loss: 73.1543 - val_mse: 73.1543\n",
            "Epoch 22/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 49.5243 - mse: 49.5243 - val_loss: 63.0193 - val_mse: 63.0193\n",
            "Epoch 23/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 41.3832 - mse: 41.3832 - val_loss: 51.5868 - val_mse: 51.5868\n",
            "Epoch 24/100\n",
            "23/23 [==============================] - 1s 63ms/step - loss: 35.5867 - mse: 35.5867 - val_loss: 61.8079 - val_mse: 61.8079\n",
            "Epoch 25/100\n",
            "23/23 [==============================] - 2s 92ms/step - loss: 33.0246 - mse: 33.0246 - val_loss: 53.3637 - val_mse: 53.3637\n",
            "Epoch 26/100\n",
            "23/23 [==============================] - 1s 61ms/step - loss: 36.5632 - mse: 36.5632 - val_loss: 83.5834 - val_mse: 83.5834\n",
            "Epoch 27/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 36.3079 - mse: 36.3079 - val_loss: 64.1847 - val_mse: 64.1847\n",
            "Epoch 28/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 31.9215 - mse: 31.9215 - val_loss: 48.3617 - val_mse: 48.3617\n",
            "Epoch 29/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 33.3797 - mse: 33.3797 - val_loss: 51.6730 - val_mse: 51.6730\n",
            "Epoch 30/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 34.6462 - mse: 34.6462 - val_loss: 52.3982 - val_mse: 52.3982\n",
            "Epoch 31/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 27.3126 - mse: 27.3126 - val_loss: 48.8255 - val_mse: 48.8255\n",
            "Epoch 32/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 25.0834 - mse: 25.0834 - val_loss: 44.5891 - val_mse: 44.5891\n",
            "Epoch 33/100\n",
            "23/23 [==============================] - 3s 141ms/step - loss: 23.4905 - mse: 23.4905 - val_loss: 48.9816 - val_mse: 48.9816\n",
            "Epoch 34/100\n",
            "23/23 [==============================] - 2s 85ms/step - loss: 19.1881 - mse: 19.1881 - val_loss: 39.3706 - val_mse: 39.3706\n",
            "Epoch 35/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 16.8557 - mse: 16.8557 - val_loss: 34.4262 - val_mse: 34.4262\n",
            "Epoch 36/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 16.6644 - mse: 16.6644 - val_loss: 37.2569 - val_mse: 37.2569\n",
            "Epoch 37/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 17.1295 - mse: 17.1295 - val_loss: 34.6195 - val_mse: 34.6195\n",
            "Epoch 38/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 18.3849 - mse: 18.3849 - val_loss: 40.4225 - val_mse: 40.4225\n",
            "Epoch 39/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 13.7008 - mse: 13.7008 - val_loss: 36.6200 - val_mse: 36.6200\n",
            "Epoch 40/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 13.6076 - mse: 13.6076 - val_loss: 42.9797 - val_mse: 42.9797\n",
            "Epoch 41/100\n",
            "23/23 [==============================] - 2s 69ms/step - loss: 14.3143 - mse: 14.3143 - val_loss: 33.5338 - val_mse: 33.5338\n",
            "Epoch 42/100\n",
            "23/23 [==============================] - 2s 86ms/step - loss: 16.4971 - mse: 16.4971 - val_loss: 44.5863 - val_mse: 44.5863\n",
            "Epoch 43/100\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 18.0840 - mse: 18.0840 - val_loss: 46.5227 - val_mse: 46.5227\n",
            "Epoch 44/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 19.3410 - mse: 19.3410 - val_loss: 39.7542 - val_mse: 39.7542\n",
            "Epoch 45/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 18.2967 - mse: 18.2967 - val_loss: 38.8776 - val_mse: 38.8776\n",
            "Epoch 46/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 17.1955 - mse: 17.1955 - val_loss: 42.3305 - val_mse: 42.3305\n",
            "Epoch 47/100\n",
            "23/23 [==============================] - 1s 52ms/step - loss: 17.6556 - mse: 17.6556 - val_loss: 27.2352 - val_mse: 27.2352\n",
            "Epoch 48/100\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 14.7896 - mse: 14.7896 - val_loss: 33.9649 - val_mse: 33.9649\n",
            "Epoch 49/100\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 15.7150 - mse: 15.7150 - val_loss: 33.5179 - val_mse: 33.5179\n",
            "Epoch 50/100\n",
            "23/23 [==============================] - 1s 62ms/step - loss: 15.2030 - mse: 15.2030 - val_loss: 33.8574 - val_mse: 33.8574\n",
            "Epoch 51/100\n",
            "23/23 [==============================] - 2s 90ms/step - loss: 12.1174 - mse: 12.1174 - val_loss: 30.0409 - val_mse: 30.0409\n",
            "Epoch 52/100\n",
            "23/23 [==============================] - 1s 59ms/step - loss: 10.6924 - mse: 10.6924 - val_loss: 35.3921 - val_mse: 35.3921\n",
            "Epoch 53/100\n",
            "23/23 [==============================] - 1s 59ms/step - loss: 9.0686 - mse: 9.0686 - val_loss: 32.2274 - val_mse: 32.2274\n",
            "Epoch 54/100\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 9.0611 - mse: 9.0611 - val_loss: 27.3904 - val_mse: 27.3904\n",
            "Epoch 55/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 8.2822 - mse: 8.2822 - val_loss: 36.8520 - val_mse: 36.8520\n",
            "Epoch 56/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 8.3613 - mse: 8.3613 - val_loss: 32.7303 - val_mse: 32.7303\n",
            "Epoch 57/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 6.4546 - mse: 6.4546 - val_loss: 29.4755 - val_mse: 29.4755\n",
            "Epoch 58/100\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 6.6555 - mse: 6.6555 - val_loss: 36.9439 - val_mse: 36.9439\n",
            "Epoch 59/100\n",
            "23/23 [==============================] - 1s 62ms/step - loss: 7.6420 - mse: 7.6420 - val_loss: 32.8417 - val_mse: 32.8417\n",
            "Epoch 60/100\n",
            "23/23 [==============================] - 2s 89ms/step - loss: 7.5636 - mse: 7.5636 - val_loss: 31.8115 - val_mse: 31.8115\n",
            "Epoch 61/100\n",
            "23/23 [==============================] - 1s 61ms/step - loss: 8.8240 - mse: 8.8240 - val_loss: 26.3050 - val_mse: 26.3050\n",
            "Epoch 62/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 8.9107 - mse: 8.9107 - val_loss: 27.3455 - val_mse: 27.3455\n",
            "Epoch 63/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 8.9411 - mse: 8.9411 - val_loss: 26.4985 - val_mse: 26.4985\n",
            "Epoch 64/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 6.9393 - mse: 6.9393 - val_loss: 28.0849 - val_mse: 28.0849\n",
            "Epoch 65/100\n",
            "23/23 [==============================] - 1s 59ms/step - loss: 6.0702 - mse: 6.0702 - val_loss: 25.8539 - val_mse: 25.8539\n",
            "Epoch 66/100\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 6.1261 - mse: 6.1261 - val_loss: 25.9038 - val_mse: 25.9038\n",
            "Epoch 67/100\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 7.2750 - mse: 7.2750 - val_loss: 32.0600 - val_mse: 32.0600\n",
            "Epoch 68/100\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 6.7571 - mse: 6.7571 - val_loss: 27.0995 - val_mse: 27.0995\n",
            "Epoch 69/100\n",
            "23/23 [==============================] - 3s 146ms/step - loss: 6.7543 - mse: 6.7543 - val_loss: 29.6782 - val_mse: 29.6782\n",
            "Epoch 70/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 6.8385 - mse: 6.8385 - val_loss: 26.2609 - val_mse: 26.2609\n",
            "Epoch 71/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 6.2815 - mse: 6.2815 - val_loss: 29.3861 - val_mse: 29.3861\n",
            "Epoch 72/100\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 7.0302 - mse: 7.0302 - val_loss: 25.5821 - val_mse: 25.5821\n",
            "Epoch 73/100\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 6.5060 - mse: 6.5060 - val_loss: 27.5475 - val_mse: 27.5475\n",
            "Epoch 74/100\n",
            "23/23 [==============================] - 1s 51ms/step - loss: 7.1730 - mse: 7.1730 - val_loss: 27.9745 - val_mse: 27.9745\n",
            "Epoch 75/100\n",
            "23/23 [==============================] - 1s 51ms/step - loss: 5.3766 - mse: 5.3766 - val_loss: 30.2687 - val_mse: 30.2687\n",
            "Epoch 76/100\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 5.2290 - mse: 5.2290 - val_loss: 26.9810 - val_mse: 26.9810\n",
            "Epoch 77/100\n",
            "23/23 [==============================] - 1s 65ms/step - loss: 3.6886 - mse: 3.6886 - val_loss: 24.7865 - val_mse: 24.7865\n",
            "Epoch 78/100\n",
            "23/23 [==============================] - 2s 90ms/step - loss: 4.5555 - mse: 4.5555 - val_loss: 28.1135 - val_mse: 28.1135\n",
            "Epoch 79/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 3.7962 - mse: 3.7962 - val_loss: 32.7651 - val_mse: 32.7651\n",
            "Epoch 80/100\n",
            "23/23 [==============================] - 1s 51ms/step - loss: 4.6799 - mse: 4.6799 - val_loss: 29.5876 - val_mse: 29.5876\n",
            "Epoch 81/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 4.5928 - mse: 4.5928 - val_loss: 29.9273 - val_mse: 29.9273\n",
            "Epoch 82/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 4.4418 - mse: 4.4418 - val_loss: 23.4183 - val_mse: 23.4183\n",
            "Epoch 83/100\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 4.5938 - mse: 4.5938 - val_loss: 26.8806 - val_mse: 26.8806\n",
            "Epoch 84/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 4.5668 - mse: 4.5668 - val_loss: 20.7293 - val_mse: 20.7293\n",
            "Epoch 85/100\n",
            "23/23 [==============================] - 1s 59ms/step - loss: 4.7318 - mse: 4.7318 - val_loss: 23.7831 - val_mse: 23.7831\n",
            "Epoch 86/100\n",
            "23/23 [==============================] - 2s 69ms/step - loss: 3.9780 - mse: 3.9780 - val_loss: 26.2647 - val_mse: 26.2647\n",
            "Epoch 87/100\n",
            "23/23 [==============================] - 2s 91ms/step - loss: 3.7514 - mse: 3.7514 - val_loss: 25.8364 - val_mse: 25.8364\n",
            "Epoch 88/100\n",
            "23/23 [==============================] - 1s 59ms/step - loss: 3.6228 - mse: 3.6228 - val_loss: 29.5085 - val_mse: 29.5085\n",
            "Epoch 89/100\n",
            "23/23 [==============================] - 1s 59ms/step - loss: 4.2296 - mse: 4.2296 - val_loss: 27.1332 - val_mse: 27.1332\n",
            "Epoch 90/100\n",
            "23/23 [==============================] - 1s 58ms/step - loss: 4.1605 - mse: 4.1605 - val_loss: 25.1858 - val_mse: 25.1858\n",
            "Epoch 91/100\n",
            "23/23 [==============================] - 1s 52ms/step - loss: 3.1364 - mse: 3.1364 - val_loss: 25.0721 - val_mse: 25.0721\n",
            "Epoch 92/100\n",
            "23/23 [==============================] - 1s 52ms/step - loss: 3.5228 - mse: 3.5228 - val_loss: 28.5500 - val_mse: 28.5500\n",
            "Epoch 93/100\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 2.7116 - mse: 2.7116 - val_loss: 22.4906 - val_mse: 22.4906\n",
            "Epoch 94/100\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 2.6985 - mse: 2.6985 - val_loss: 28.0920 - val_mse: 28.0920\n",
            "Epoch 95/100\n",
            "23/23 [==============================] - 1s 65ms/step - loss: 3.5778 - mse: 3.5778 - val_loss: 31.9507 - val_mse: 31.9507\n",
            "Epoch 96/100\n",
            "23/23 [==============================] - 2s 90ms/step - loss: 4.1297 - mse: 4.1297 - val_loss: 28.1104 - val_mse: 28.1104\n",
            "Epoch 97/100\n",
            "23/23 [==============================] - 1s 62ms/step - loss: 6.8570 - mse: 6.8570 - val_loss: 25.0786 - val_mse: 25.0786\n",
            "Epoch 98/100\n",
            "23/23 [==============================] - 1s 59ms/step - loss: 8.7480 - mse: 8.7480 - val_loss: 29.9237 - val_mse: 29.9237\n",
            "Epoch 99/100\n",
            "23/23 [==============================] - 1s 57ms/step - loss: 5.6320 - mse: 5.6320 - val_loss: 28.5151 - val_mse: 28.5151\n",
            "Epoch 100/100\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 3.6628 - mse: 3.6628 - val_loss: 26.4175 - val_mse: 26.4175\n",
            "3/3 [==============================] - 0s 11ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 12ms/step\n",
            "3/3 [==============================] - 0s 12ms/step\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000288 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9230\n",
            "[LightGBM] [Info] Number of data points in the train set: 720, number of used features: 59\n",
            "[LightGBM] [Info] Start training from score 38.939428\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "2\n",
            "Mean Absolute Error: 2 149.97 3.464366009218606 5.139793307634591 0.9711612678803443\n",
            "Mean Absolute Error: 2 1.75 3.4606099995001927 4.955569468959956 0.9731915323350177\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from keras.layers import (Input, Reshape)\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPool2D,BatchNormalization,Activation\n",
        "from keras.layers import SeparableConv2D\n",
        "from keras import backend as K\n",
        "from keras.layers import (Input, Reshape)\n",
        "from keras.layers import Concatenate\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.models import Model\n",
        "import lightgbm as lgb\n",
        "from PIL import Image\n",
        "X= np.loadtxt(open(\"/x(1).csv\",\"rb\"),delimiter=\",\",skiprows=1)\n",
        "y=np.loadtxt(open(\"/y(1).csv\",\"rb\"),delimiter=\",\",skiprows=1)\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 加载图像并预处理\n",
        "img = Image.open('/background.png')\n",
        "img_resized = img.resize((16, 16))\n",
        "img_gray = img_resized.convert('L')  # 如果模型接受灰度图像\n",
        "img_array = np.array(img_gray)\n",
        "img_array = img_array / 255.0  # 归一化\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = np.expand_dims(img_array, axis=-1)  # 如果模型接受单通道图像\n",
        "\n",
        "\n",
        "def compute_gradcam(model, img_array, layer_name):\n",
        "    # 构建一个模型，其输出是指定层的输出和最后的模型输出\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(layer_name).output, model.output])\n",
        "\n",
        "    # 记录指定层的梯度\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        loss = predictions\n",
        "\n",
        "    # 获取指定层的输出\n",
        "    output = conv_outputs[0]\n",
        "    grads = tape.gradient(loss, conv_outputs)[0]\n",
        "\n",
        "    # 计算梯度的加权平均\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # 将特征图数组和梯度数组相乘，然后取平均\n",
        "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, output), axis=-1)\n",
        "\n",
        "    # ReLU激活并归一化热力图\n",
        "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "    return heatmap\n",
        "\n",
        "\n",
        "i=1\n",
        "time12=[]\n",
        "mae12=[]\n",
        "rmse12=[]\n",
        "r212=[]\n",
        "time22=[]\n",
        "mae22=[]\n",
        "rmse22=[]\n",
        "r222=[]\n",
        "while i<=1:\n",
        "\n",
        "    import time\n",
        "    start1 = time.time()\n",
        "    data_y, data_yhat = list(), list()\n",
        "    kfold = KFold(n_splits=10, shuffle=True)\n",
        "# enumerate splits\n",
        "    for train_ix, test_ix in kfold.split(X):\n",
        "# get data\n",
        "        train_x, test_x = X[train_ix], X[test_ix]\n",
        "        train_y, test_y = y[train_ix], y[test_ix]\n",
        "    ss_x = preprocessing.StandardScaler()\n",
        "    train_x_disorder = ss_x.fit_transform(train_x)\n",
        "    test_x_disorder = ss_x.transform(test_x)\n",
        "    X_train=train_x_disorder.reshape([-1,16,16, 1])\n",
        "    X_test=test_x_disorder.reshape([-1, 16,16, 1])\n",
        "    y_train=train_y.reshape(-1, 1)\n",
        "    y_test=test_y.reshape(-1, 1)\n",
        "\n",
        "    inputs=Input(shape=(16,16,1))\n",
        "\n",
        "    x1=keras.layers.Conv2D(16,(1,1),activation='relu',padding='SAME')(inputs)\n",
        "    x2=keras.layers.Conv2D(32,(1,1),activation='relu',padding='SAME')(x1)\n",
        "    x3=keras.layers.Conv2D(64,(1,1),activation='relu',padding='SAME')(x2)\n",
        "    x4=keras.layers.Conv2D(128,(1,1),activation='relu',padding='SAME')(x3)\n",
        "    x2=keras.layers.Conv2D(16,(1,1),activation='relu',padding='SAME')(x2)\n",
        "    x3=keras.layers.Conv2D(16,(1,1),activation='relu',padding='SAME')(x3)\n",
        "    x4=keras.layers.Conv2D(16,(1,1),activation='relu',padding='SAME')(x4)\n",
        "\n",
        "    x4_3 = abs(x4-x3)\n",
        "    x3_2 = abs(x3-x2)\n",
        "    x2_1 = abs(x2-x1)\n",
        "\n",
        "    x4_3_2 = abs(x4_3 - x3_2)\n",
        "    x3_2_1 = abs(x3_2 - x2_1)\n",
        "\n",
        "    x4_3_2_1 = abs(x4_3_2 - x3_2_1)\n",
        "\n",
        "    level4=x4\n",
        "    level3 = x4_3\n",
        "    level2 = x3_2 + x4_3_2\n",
        "    level1 =x2_1 + x3_2_1 + x4_3_2_1\n",
        "\n",
        "    #x4_dem_4 = x4\n",
        "    #output3 = x4_dem_4 + level3\n",
        "    #output2 = output3 + level2\n",
        "    #output1 = output2 + level1\n",
        "    x1_mean=keras.layers.GlobalAveragePooling2D()(level1)\n",
        "    x2_mean=keras.layers.GlobalAveragePooling2D()(level2)\n",
        "    x3_mean=keras.layers.GlobalAveragePooling2D()(level3)\n",
        "    x4_mean=keras.layers.GlobalAveragePooling2D()(level4)\n",
        "\n",
        "    print('x1:',x1.shape,x1_mean.shape,x4.shape,x4_mean.shape)\n",
        "\n",
        "    x1_weight=keras.layers.Dense(units=x1.shape[-1], activation='sigmoid')(x1_mean)\n",
        "    x2_weight=keras.layers.Dense(units=x2.shape[-1], activation='sigmoid')(x2_mean)\n",
        "    x3_weight=keras.layers.Dense(units=x3.shape[-1], activation='sigmoid')(x3_mean)\n",
        "    x4_weight=keras.layers.Dense(units=x4.shape[-1], activation='sigmoid')(x4_mean)\n",
        "    print('x1_weight:',x1_weight.shape,x4_weight,x4_weight.shape)\n",
        "\n",
        "    x1_weight=keras.layers.Reshape((1,1,x1.shape[-1]))(x1_weight)\n",
        "    x2_weight=keras.layers.Reshape((1,1,x2.shape[-1]))(x2_weight)\n",
        "    x3_weight=keras.layers.Reshape((1,1,x3.shape[-1]))(x3_weight)\n",
        "    x4_weight=keras.layers.Reshape((1,1,x4.shape[-1]))(x4_weight)\n",
        "    print('x1_weight:',x1_weight.shape,x4_weight,x4_weight.shape)\n",
        "\n",
        "    x1_weighted=level1*x1_weight\n",
        "    x2_weighted=level2*x2_weight\n",
        "    x3_weighted=level3*x3_weight\n",
        "    x4_weighted=level4*x4_weight\n",
        "    print('x1_weighted:',x1_weighted.shape,x4_weighted,x4_weighted.shape)\n",
        "\n",
        "    #x=keras.layers.Concatenate(axis=-1)([x1_weighted, x2_weighted, x3_weighted, x4_weighted])\n",
        "    x=x1_weighted+x2_weighted+x3_weighted+x4_weighted\n",
        "    print(x.shape)\n",
        "    x=keras.layers.Flatten()(x)\n",
        "    x=keras.layers.Dense(256,activation='relu',name='mydense1')(x)\n",
        "    x=keras.layers.Dense(120,activation='relu',name='mydense2')(x)\n",
        "    x=keras.layers.Dense(100,activation='relu',name='mydense3')(x)\n",
        "    #=keras.layers.Dense(40,activation='relu',name='mydense5')(x)\n",
        "    end1 = time.time()\n",
        "    x=keras.layers.Dense(1,activation='linear')(x)\n",
        "    model=keras.Model(inputs,x)\n",
        "    model.compile(optimizer=\"adam\",loss='mean_squared_error',metrics=[\"mse\"])\n",
        "    model_log = model.fit(X_train,y_train,batch_size=32,epochs=100,validation_data=(X_test,y_test),shuffle=True)\n",
        "    y_pred1 = model.predict(X_test)  #根据输入数据进行预测的实际值\n",
        "    end2 = time.time()\n",
        "    from sklearn import metrics\n",
        "    time1=\"%.2f\"%(end2-start1)\n",
        "    mae1=metrics.mean_absolute_error(test_y, y_pred1)\n",
        "    rmse1=np.sqrt(metrics.mean_squared_error(test_y, y_pred1))\n",
        "    r21=metrics.r2_score(test_y, y_pred1)\n",
        "    time12.append(float(time1))\n",
        "    mae12.append(float(mae1))\n",
        "    rmse12.append(float(rmse1))\n",
        "    r212.append(float(r21))\n",
        "    heatmap = compute_gradcam(model, img_array, 'tf.__operators__.add_93')\n",
        "\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "\n",
        "# 假设 'heatmap' 是通过 Grad-CAM 计算得到的热力图\n",
        "# 假设 'img' 是 PIL 图像格式的原始图像\n",
        "\n",
        "# 将 PIL 图像转换为 NumPy 数组以用于绘图\n",
        "    img_np = np.array(img)\n",
        "\n",
        "# 调整热力图大小以匹配原始图像\n",
        "    heatmap_resized = np.array(Image.fromarray(np.uint8(plt.cm.jet(heatmap)*255)).resize(img_np.shape[1::-1], Image.LANCZOS))\n",
        "\n",
        "# 将热力图应用于原始图像\n",
        "    heatmap_on_img = np.uint8(heatmap_resized[..., :3] * 0.5 + img_np * 0.5)\n",
        "\n",
        "# 显示原始图像和叠加后的热力图\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title('Original Image')\n",
        "    plt.imshow(img_np)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title('Grad-CAM')\n",
        "    plt.imshow(heatmap_on_img)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    start2 = time.time()\n",
        "    model_mydense_output3 = Model(inputs=model.input,outputs=model.get_layer('mydense3').output)\n",
        "    X_tr3=model_mydense_output3.predict(X_train)\n",
        "    X_te3=model_mydense_output3.predict(X_test)\n",
        "    model1 =lgb.LGBMRegressor()\n",
        "    evals_result = {}\n",
        "    # define the datasets to evaluate each iteration\n",
        "    evalset = [(X_tr3, y_train), (X_te3,y_test)]\n",
        "    model1 = lgb.LGBMRegressor()\n",
        "    # fit the model\n",
        "    model1 = model1.fit(X_tr3, y_train)\n",
        "    y_pred2 = model1.predict(X_te3)\n",
        "    end3 = time.time()\n",
        "    time21=\"%.2f\"%(end3-start2+end1-start1)\n",
        "    mae21=metrics.mean_absolute_error(test_y, y_pred2)\n",
        "    rmse21=np.sqrt(metrics.mean_squared_error(test_y, y_pred2))\n",
        "    r221=metrics.r2_score(test_y, y_pred2)\n",
        "    time22.append(float(time21))\n",
        "    mae22.append(float(mae21))\n",
        "    rmse22.append(float(rmse21))\n",
        "    r222.append(float(r221))\n",
        "\n",
        "    i=i+1\n",
        "    print(i)\n",
        "    print('Mean Absolute Error:',i,np.mean(time12),np.mean(mae12),np.mean(rmse12),np.mean(r212))\n",
        "print('Mean Absolute Error:',i,np.mean(time22),np.mean(mae22),np.mean(rmse22),np.mean(r222))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SnD2MMzWD4_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IwPo4zgjD5Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KvRONXVuDx0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9Nn-SLzSk80Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "XVCBWJCBikDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "X = np.loadtxt(\"/x(1).csv\", delimiter=\",\", skiprows=1)\n",
        "y = np.loadtxt(\"/y(1).csv\", delimiter=\",\", skiprows=1)\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "# 数据标准化\n",
        "ss_x = StandardScaler()\n",
        "X_scaled = ss_x.fit_transform(X)\n",
        "X_scaled = X_scaled.reshape(-1, 16, 16, 1)\n",
        "\n",
        "# 将numpy数组转换为torch张量\n",
        "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "i=1\n",
        "time12=[]\n",
        "mae12=[]\n",
        "rmse12=[]\n",
        "r212=[]\n",
        "time22=[]\n",
        "mae22=[]\n",
        "rmse22=[]\n",
        "r222=[]\n",
        "while i<=1:\n",
        "\n",
        "    import time\n",
        "    start1 = time.time()\n",
        "    data_y, data_yhat = list(), list()\n",
        "# 加载数据\n",
        "\n",
        "# K折交叉验证\n",
        "\n",
        "\n",
        "\n",
        "class MSMWGBMNet(nn.Module)\n",
        "\n"
      ],
      "metadata": {
        "id": "lpzZY6SVEz7G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}